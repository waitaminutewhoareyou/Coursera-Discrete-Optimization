{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MITx Clinical NLP for de-identification",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WittgensteinInHisYouth/Coursera-Discrete-Optimization/blob/master/MITx_Clinical_NLP_for_de_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loZ-Kj3qPDtb"
      },
      "source": [
        "# Clinical NLP for De-identification\n",
        "\n",
        "## Task\n",
        "As outlined in the problem overview, in this problem we will be working with the n2c2 2014 challenge, de-identification track data. This challenge tasks participants to design models to ingest clinical notes and identify regions of Personal Health Information (PHI), so that they can be automatically obfuscated by de-identification systems. For example, if a note said \n",
        "\n",
        "  > Dr. Pompies perscribed Deniz Aslan a daily antibiotic after a visit to The Hospital for Doctoring\n",
        "\n",
        "then the system should flag \"Pompies\", \"Deniz Aslan\" as names for obfuscation, and \"The Hospital for Doctoring\" as the name of a hospital for obfuscation. We work with these labels via a variant of the [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) system, where we tag the _beginnings_ of spans of PHI with a \"B-\" prefixed label (the label suffix indicates what kind of PHI the span is), the _inside_ of spans of PHI with an \"I-\" tag, and all non-PHI tokens (the _outside_ of spans of PHI) with an \"O\" label. This format is commonly used in clinical NER tasks. The labels for our example sentence above would thus be\n",
        "\n",
        "  > O B-DOCTOR O B-PATIENT I-PATIENT O O O O O O O B-HOSPITAL I-HOSPITAL I-HOSPITAL I-HOSPITAL\n",
        "\n",
        "Note that while the data we use in this problem does come from the n2c2 task, for reasons of technical expedience we don't evaluate in a manner consistent with the official challenge evaluation,so your results will not be consistent with published numbers.\n",
        "\n",
        "### Outline\n",
        "In this notebook, first, we'll have some imports and helper functions, then we'll work through some data exploration and baseline methods, followed by methods using recurrent neural networks to acheive better performance.\n",
        "\n",
        "## Import, constants, and helper functions\n",
        "Do not modify this section. Note that after exectuing the first cell, you will need to restart the runtime for the package installations to take effect. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkg3HaRpvjSq"
      },
      "source": [
        "#@title Installing Packages\n",
        "##Note that this takes a while to run! \n",
        "\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "!pip install allennlp\n",
        "!pip install unidecode\n",
        "\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import torch, torch.nn as nn, torch.utils.data as data, torch.optim as optim\n",
        "from allennlp.modules.time_distributed import TimeDistributed\n",
        "\n",
        "from google.colab import auth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDBvUlhW0mby",
        "cellView": "form"
      },
      "source": [
        "#@title Defining Constants, fixing random seeds\n",
        "\n",
        "RARE_WORDS_THRESHOLD = 4\n",
        "UNK = \" _ UNK _ \"\n",
        "PAD = \" _ PAD _ \"\n",
        "PUNCT = \"PUNCTUATION\"\n",
        "NUM = \"#\"\n",
        "\n",
        "#Fixing random seeds\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duq4QV0o2Rf1",
        "cellView": "form"
      },
      "source": [
        "#@title Defining read_dataset_file(), train, dev and test\n",
        "\n",
        "def read_dataset_file(filepath):\n",
        "  with open(filepath, mode='r') as f: lines = f.readlines()\n",
        "  out, sentence = [], []\n",
        "  for line in lines:\n",
        "    if not line.strip(): \n",
        "      out.append(sentence)\n",
        "      sentence = []\n",
        "    else: \n",
        "      word, label = line.strip().split()\n",
        "      sentence.append((word, label))\n",
        "  return out\n",
        "\n",
        "train = read_dataset_file('2014_train.tsv')\n",
        "dev = read_dataset_file('2014_dev.tsv')\n",
        "test = read_dataset_file('2014_test.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxvoCoeUvVX2",
        "cellView": "form"
      },
      "source": [
        "#@title Defining ner_eval() which serves as an evaluation script\n",
        "\n",
        "def ner_eval(\n",
        "    probs, true, label_vocab, metric='Macro F1',\n",
        "):\n",
        "  \"\"\"\n",
        "  Simple, *NON-OFFICIAL* evaluation script. \n",
        "  probs: [ sentence 1: [[probabilities_per_label for word 1], ... ], ...]\n",
        "  true: [ sentence 1: [(word_1, label_1), ...], ...]\n",
        "  label_vocab: [labels in order of elements of probability vectors.]\n",
        "  metric = {'F1', ...}\n",
        "  \"\"\"\n",
        "  label_idxmap = {l: i for i, l in enumerate(label_vocab)}\n",
        "\n",
        "  assert metric == 'Macro F1', \"Metric %s not supported\" % metric\n",
        "\n",
        "  if metric == 'Macro F1':\n",
        "    flat_preds = [probs.argmax() for sent in probs for probs in sent]\n",
        "    flat_true = [label_idxmap[\n",
        "        l if l in label_idxmap else PAD\n",
        "    ] for sent in true for _, l in sent]\n",
        "    score = f1_score(flat_true, flat_preds, average='macro')\n",
        "  \n",
        "  return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvKex6ymPBuv"
      },
      "source": [
        "## Dataset Exploration\n",
        "Now that we've loaded our data, let's take a look at its format.\n",
        "\n",
        "### Examining the Data Directly\n",
        "What can we see here about this data? We can observe both technical properties, about how the data is organized, and conceptual factors, such as the kinds of labels present in the data.\n",
        "\n",
        "#### Technical Factors\n",
        "Let's print some stats about our dataset, like its size, and show some examples to see if we can understand the format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFpPgmPebqAN"
      },
      "source": [
        "###Printing stats about our dataset, size and examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrKHptKXtagb"
      },
      "source": [
        "###Printing stats about our dataset, size and examples\n",
        "\n",
        "train_len = len(train)\n",
        "dev_len = len(dev)\n",
        "test_len = len(test)\n",
        "\n",
        "train_lens = [len(s) for s in train]\n",
        "dev_lens = [len(s) for s in dev]\n",
        "\n",
        "print(\n",
        "    \"Train contains %d sentences, ranging from %d - %d (avg %d) words long.\"\n",
        "    \"\" % (train_len, min(train_lens), max(train_lens), np.mean(train_lens))\n",
        ")\n",
        "print(\"Train Samples:\")\n",
        "print(train[0])\n",
        "print(train[1])\n",
        "\n",
        "print(\n",
        "    \"Dev contains %d sentences, ranging from %d - %d (avg %d) words long.\"\n",
        "    \"\" % (dev_len, min(dev_lens), max(dev_lens), np.mean(dev_lens))\n",
        ")\n",
        "print(\"Test contains %d sentences.\" % test_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2BBo7B2cKa9"
      },
      "source": [
        "It appears our data-loaders have loaded the files in such a way that `train` or `dev` contains a list, where each element of the list corresponds to a sentence or line in a clinical note, represented as a list of tuples, each containing first the _word_ at that position in the sentence, followed by the _label_ of that word. Note that the underlying file format we use, if you choose to inspect that, is a common format for use with the BERT model and NER (though we won't use BERT in this assignment).\n",
        "\n",
        "Note the size discrepancy between our dev dataset and test dataset. In general, seeing such a large difference would be cause for concern -- perhaps these were not sampled in an iid manner, but instead obtained from different sources and this explains the delta in size. Here, however, it is merely that the original source data for this task used a 60/40 train-test split, which is abnormally large, and our selected dev set (randomly chosen from the original data) is only a 10% split. See here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4989908/ for more details\n",
        "\n",
        "#### Labels present\n",
        "Stripping off the IOB format prefixes (`B-`, `I-`), we see that we have 24 PHI categories, including non-PHI (`O`). Normally, you'd need to investigate the data generative process, or speak to your clinical collaborators to understand the exact meaning of these labels, but in this case, you can refer to the published paper describing this dataset for more information on the meaning of each of these labels: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4989908/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DLn3f6mcACF"
      },
      "source": [
        "###Printing labels present in dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BYoFkS_cbZ-"
      },
      "source": [
        "train_labels = set(l for sent in train for w, l in sent)\n",
        "# Note that, in reality, we won't be able to use the labels we add below from\n",
        "# dev in any model decisions if it is to truly simulate a held out test set.\n",
        "dev_labels = set(l for sent in dev for w, l in sent)\n",
        "test_labels = set(l for sent in read_dataset_file('2014_test.tsv') for w, l in sent)\n",
        "\n",
        "print(dev_labels - train_labels)\n",
        "print(test_labels - train_labels)\n",
        "\n",
        "all_labels = train_labels\n",
        "all_labels.update(dev_labels)\n",
        "all_labels.update(test_labels)\n",
        "\n",
        "phi_categories = set(l if l == 'O' else l[2:] for l in all_labels)\n",
        "phi_category_counts = Counter(\n",
        "    (l if l == 'O' else l[2:]) for sent in train for _, l in sent\n",
        ")\n",
        "print(\n",
        "    \"All %d PHI Types: \\n\"\n",
        "    \"%s\"\n",
        "    \"\" % (\n",
        "        len(phi_categories),\n",
        "        '\\n'.join('  %s: %d' % e for e in phi_category_counts.most_common())\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QpEe6lRby9"
      },
      "source": [
        "### Word Frequencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqkJzX0Z0gMa",
        "cellView": "form"
      },
      "source": [
        "#@title Counting words with occurrences less than RARE_WORDS_THRESHOLD\n",
        "words_and_labels = Counter(w_and_l for sent in train for w_and_l in sent)\n",
        "vocab = Counter(w for sent in train for w, l in sent)\n",
        "labels = Counter(l for sent in train for w, l in sent)\n",
        "\n",
        "rare_words = Counter({w: cnt for w, cnt in vocab.items() if cnt < RARE_WORDS_THRESHOLD})\n",
        "rest_words = Counter({w: cnt for w, cnt in vocab.items() if cnt >= RARE_WORDS_THRESHOLD})\n",
        "\n",
        "rare_words_aggregated = {\n",
        "    l: sum(words_and_labels[(w, l)] for w in rare_words) for l in labels\n",
        "}\n",
        "\n",
        "print(\n",
        "    \"Extracted %d rare words of %d total (%.2f%%)\" % (\n",
        "        len(rare_words), len(vocab), len(rare_words)/len(vocab) * 100\n",
        "    )\n",
        ")\n",
        "print(\"Sample Rare words\", list(rare_words.items())[:5])\n",
        "print(\"Sample Common words\", list(rest_words.items())[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYGMuxq0vXcX"
      },
      "source": [
        "## Baselines\n",
        "### A Simple, Unigram Baseline\n",
        "First, we'll experiment with a simple, per-word baseline. Normally, we might consider making a linear model with a unigram, bag-of-words baseline as our simplest baseline. However, some consideration points out that a linear model with _unigram_ inputs is simply asking the model to assign optimal probabilities to each word indepedently, and these we can compute more efficiently, so we'll do that here as our simplest baseline. All we need to do is count how often each label is assigned to any unique word in our train set (aggregating together all rare words into an \"UNKNOWN\" bucket so we can still predict on unseen words at test time), then normalize by word to get probabilities. Note that if you change the `RARE_WORDS_THRESHOLD` variable, this cell may take a long time to compute, but at the default parameters, should take only seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvG0awTvyBTc"
      },
      "source": [
        "%%time\n",
        "all_data = [(w,l,cnt) for (w,l), cnt in words_and_labels.items() if w in rest_words]\n",
        "for label in labels.keys():\n",
        "  all_data.append((UNK, label, rare_words_aggregated[label]))\n",
        "\n",
        "labeling_rates = pd.DataFrame(all_data, columns = ('word', 'label', 'count'))\n",
        "\n",
        "labeling_rates = labeling_rates.pivot_table(\n",
        "    index='word', columns='label', values='count'\n",
        ")\n",
        "labeling_rates.fillna(0, inplace=True)\n",
        "\n",
        "# Convert counts into per-word probabilities\n",
        "labeling_rates = labeling_rates.div(labeling_rates.sum(axis=1), axis='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAnMPV5oehDT"
      },
      "source": [
        "What does our output look like? A table with word as our index key, and each row giving the probabilities for all label types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RTRMnFPBKqC"
      },
      "source": [
        "labeling_rates.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8m1l1MAerGT"
      },
      "source": [
        "Hence we see that our output is a table with word as our index key, and each row giving the probabilities for all label types. Let's also plot the labeling rates of rare words here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "YQz1Vbl2z8H8"
      },
      "source": [
        "#@title Plotting labelling rates for rare words\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "data = labeling_rates.loc[UNK].sort_values()\n",
        "ax.bar(data.index, data.values)\n",
        "ax.tick_params(rotation=90, axis='x')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2pt4cVGfHr4"
      },
      "source": [
        "Now we can make some actual predictions on the validation dataset and evaluate via our per-word macro F1 metric:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gviqyIFUvVaU"
      },
      "source": [
        "##Evaluating unigram baseline on validation dataset\n",
        "%%time\n",
        "dev_probs = []\n",
        "for sent in dev:\n",
        "  sent_probs = []\n",
        "  for w, _ in sent:\n",
        "    w = w if w in rest_words else UNK\n",
        "    probabilities = labeling_rates.loc[w]\n",
        "    sent_probs.append(probabilities.values)\n",
        "  dev_probs.append(sent_probs)\n",
        "\n",
        "unigram_dev_F1 = ner_eval(dev_probs, dev, list(labeling_rates.columns) + [PAD])\n",
        "print(\"We obtain a Dev F1 of %.2f\" % unigram_dev_F1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ai48gkawx6A"
      },
      "source": [
        "##TODO: Problem 2.1 - report the test F1 score."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9QtfnTJYbkz"
      },
      "source": [
        "### An Improved Baseline\n",
        "As far as Macro-F1s go, this isn't great. Note that this isn't a comparable metric to the true SOTA for this task, which uses an exact F1 measure comparing predicted spans, as opposed to our crude, per-word metric here. But still, we can do better.\n",
        "\n",
        "We'll now work through a more involved baseline, which will incorporate 3 primary changes:\n",
        "  1. An improved tokenization of the data.\n",
        "  2. An n-gram model incorporating context words.\n",
        "  3. Additional syntactic features capturing punctuation/stylistic signals that may be indicative of PHI\n",
        "\n",
        "  Note that there are other changes possible that may also yield a much more competitive baseline. Ultimately, your goal in this part of this problem will be to analyze both of these baselines, understand what changes drive the performance discrepancy between them, and how various other settings will affect final performance.\n",
        "\n",
        "#### Re-tokenization\n",
        "Here, we'll write our own tokenizer, though in practice, there are numerous libraries that one _should_ use instead. [spaCy](https://spacy.io/) is a common goto, though for simple baselines, [`scikit-learn`'s Vectorizers](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text) have most of the features we include in our tokenizer, plus some extras (stop word removal). We write our own here for 3 reasons:\n",
        "  1. spaCy is heavy, and requires downloading language resource packs, which seemed an unnecessary complication for a homework.\n",
        "  2. `scikit-learn`'s vectorizers are better suited to whole-document or whole-sentence classification, rather than NER, as we do here.\n",
        "  3. We'll use this improved tokenization for our neural network model below as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnbijHpTvVct",
        "cellView": "form"
      },
      "source": [
        "#@title Defining our own class Tokenizer()\n",
        "from enum import Enum, auto\n",
        "import copy, string, re, unidecode\n",
        "\n",
        "class AuxiliaryFeatures(Enum):\n",
        "  ENDS_IN_COLON = auto()\n",
        "  IS_CAPITALIZED = auto()\n",
        "  ALL_CAPITALIZED = auto()\n",
        "\n",
        "class Tokenizer():\n",
        "  FEATURE_INDICATORS = {\n",
        "    AuxiliaryFeatures.ENDS_IN_COLON: lambda w: w[-1] == ':',\n",
        "    AuxiliaryFeatures.IS_CAPITALIZED: lambda w: w[0].isupper(),\n",
        "    AuxiliaryFeatures.ALL_CAPITALIZED: lambda w: w.isupper(),\n",
        "  }\n",
        "\n",
        "  def __init__(\n",
        "      self, rare_words_thresh=RARE_WORDS_THRESHOLD,\n",
        "      lowercase=True,\n",
        "      remove_punct=True,\n",
        "      replace_only_punct=PUNCT,\n",
        "      remove_empty_words=True,\n",
        "      replace_numbers=NUM,\n",
        "      auxiliary_features=[\n",
        "        AuxiliaryFeatures.ENDS_IN_COLON, AuxiliaryFeatures.IS_CAPITALIZED\n",
        "      ],\n",
        "      add_pad_label=True,\n",
        "      add_pad_token=True,\n",
        "  ):\n",
        "    self.rare_words_thresh  = rare_words_thresh\n",
        "    self.auxiliary_features = set(auxiliary_features)\n",
        "    self.is_fit = False\n",
        "\n",
        "    self.lowercase          = lowercase\n",
        "    \n",
        "    if remove_punct == True: remove_punct = string.punctuation\n",
        "    self.remove_punct       = remove_punct\n",
        "    self.replace_only_punct = replace_only_punct\n",
        "    self.replace_numbers    = replace_numbers\n",
        "    self.remove_empty_words = remove_empty_words\n",
        "    self.add_pad_label      = add_pad_label\n",
        "    self.add_pad_token      = add_pad_token\n",
        "\n",
        "  def __normalize_dataset(self, dataset):\n",
        "    norm_dataset = []\n",
        "    removed_non_Os = {}\n",
        "    for sent in dataset:\n",
        "      norm_sent = []\n",
        "      for w_orig, l in sent:\n",
        "        w = unidecode.unidecode(w_orig).strip()\n",
        "        if self.lowercase: w = w.lower()\n",
        "        if self.remove_punct:\n",
        "          w = w.translate(str.maketrans('', '', self.remove_punct))\n",
        "          if w_orig and not w and self.replace_only_punct:\n",
        "            w = self.replace_only_punct\n",
        "        if self.replace_numbers:\n",
        "          # This must happen after remove punctuation.\n",
        "          w = re.sub(r'\\d+', NUM, w)\n",
        "\n",
        "        w = w.strip()\n",
        "\n",
        "        if w or (not self.remove_empty_words):\n",
        "          token_feats = []\n",
        "          for aux in self.auxiliary_features:\n",
        "            token_feats.append(float(Tokenizer.FEATURE_INDICATORS[aux](w_orig)))\n",
        "          norm_sent.append(((w, tuple(token_feats)), l))\n",
        "        else:\n",
        "          if l != 'O':\n",
        "            if w_orig not in removed_non_Os:\n",
        "              removed_non_Os[w_orig] = 1\n",
        "              print(\"WARNING: Removing non-'O' token: \\\"%s\\\"!\" % w_orig)\n",
        "            else: removed_non_Os[w_orig] += 1\n",
        "      norm_dataset.append(norm_sent)\n",
        "    return norm_dataset\n",
        "\n",
        "  def __build_vocabs(self, dataset):\n",
        "    # Labels:\n",
        "    self.labels = Counter(l for sent in dataset for w, l in sent)\n",
        "    self.labels_list = [l for l, _ in self.labels.most_common()]\n",
        "    if self.add_pad_label: self.labels_list.append(PAD)\n",
        "\n",
        "    self.labels_idxmap = {l: i for i, l in enumerate(self.labels_list)}\n",
        "    self.num_classes = len(self.labels_list)\n",
        "\n",
        "    # Word Level Vocabs:\n",
        "    # What's our raw, unfiltered vocabulary? Here we just grab the words\n",
        "    # themselves, no auxiliary features.\n",
        "    self.raw_vocab = Counter(w[0] for sent in dataset for w, l in sent)\n",
        "\n",
        "    # Filter out the rare words:\n",
        "    self.rare_words = set(\n",
        "        w for w, cnt in self.raw_vocab.items() if cnt < self.rare_words_thresh\n",
        "    )\n",
        "    self.seen_words = set(\n",
        "        w for w in self.raw_vocab if w not in self.rare_words\n",
        "    )\n",
        "    self.vocab = Counter(\n",
        "        {w: cnt for w, cnt in self.raw_vocab.items() if w in self.seen_words}\n",
        "    )\n",
        "    self.vocab[UNK] = sum(self.raw_vocab[w] for w in self.rare_words)\n",
        "\n",
        "    # Make our idxmaps, for quickly mapping from a word to its numerical index.\n",
        "    self.vocab_list = [w for w, _ in self.vocab.most_common()]\n",
        "    if self.add_pad_token:\n",
        "      self.vocab_list = [PAD] + self.vocab_list\n",
        "      self.vocab[PAD] = 0\n",
        "\n",
        "    self.vocab_idxmap = {w: i for i, w in enumerate(self.vocab_list)}\n",
        "\n",
        "    self.vocab_size = len(self.vocab_list)\n",
        "\n",
        "    # Character Level Vocab:\n",
        "    self.chars = Counter(c for w in self.raw_vocab.keys() for c in w)\n",
        "    self.chars_list = [PAD, UNK] + [c for c, _ in self.chars.most_common()]\n",
        "    self.chars_idxmap = {c: i for i, c in enumerate(self.chars_list)}\n",
        "    self.chars_vocab_size = len(self.chars_list)\n",
        "\n",
        "  def fit(self, dataset):\n",
        "    # transformed_dataset\n",
        "    normalized_dataset = self.__normalize_dataset(copy.copy(dataset))\n",
        "    self.__build_vocabs(normalized_dataset)\n",
        "    self.is_fit = True\n",
        "\n",
        "  def transform(self, dataset, as_index=True):\n",
        "    \"\"\"\n",
        "    dataset is [ (sentence: [(word, label), ...]), ...]\n",
        "    \"\"\"\n",
        "    assert self.is_fit, \"Can't transform without being fit on train data!\"\n",
        "\n",
        "    normalized_dataset = self.__normalize_dataset(copy.copy(dataset))\n",
        "    if not as_index: return normalized_dataset\n",
        "\n",
        "    indexed_dataset = []\n",
        "    for sent in normalized_dataset:\n",
        "      indexed_sent = []\n",
        "      for w, l in sent:\n",
        "        w_idx = self.vocab_idxmap[w[0] if w[0] in self.vocab else UNK]\n",
        "        indexed_sent.append((tuple([w_idx] + list(w[1:])), l))\n",
        "      indexed_dataset.append(indexed_sent)\n",
        "    \n",
        "    return indexed_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx7YwONHlapT"
      },
      "source": [
        "##Fitting tokenizer to training dataset\n",
        "tokenizer = Tokenizer(rare_words_thresh=10)\n",
        "tokenizer.fit(train)\n",
        "\n",
        "train_tokenized_no_index = tokenizer.transform(train, as_index=False)\n",
        "train_tokenized_index = tokenizer.transform(train, as_index=True)\n",
        "for sent_num in range(2):\n",
        "  print(\"Original (%d)\" % sent_num, train[sent_num][:15], \"...\")\n",
        "  print(\"New (non-indexified)\", train_tokenized_no_index[sent_num][:15], \"...\")\n",
        "  print(\"New (indexified)\", train_tokenized_index[sent_num][:15], \"...\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "print(\n",
        "    \"The new tokenizer contains %d tokens (omitting %d rare):\" % (\n",
        "        tokenizer.vocab_size, len(tokenizer.rare_words)\n",
        "    ),\n",
        "    tokenizer.vocab_list[:25]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R29r-mAIpML2"
      },
      "source": [
        "This still very basic tokenizer has _dramatically_ reduced our vocabulary size, from 14k/35k non-rare/rare words to 4k/15k, respectively (and, note that here we're being much more aggressive with the words we keep, aggregating words that appear less than 15 times).\n",
        "\n",
        "However, tokenization is still only the first step. Now, we need to build our real training corpus.\n",
        "\n",
        "### An _n_-gram, Auxiliary Features Baseline\n",
        "Our new model will be a logistic regression model trained using trigram information, concatenating the target word with the prior word and the subsequent word, along with all three word's auxiliary features (capitalization status and use of colons)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Uub8f5r-oI"
      },
      "source": [
        "##generating onehot embeddings for ngrams\n",
        "def to_onehot_ngram(\n",
        "    indexed_dataset, n_pre=1, n_post=1, vocab_idxmap=tokenizer.vocab_idxmap,\n",
        "    labels_idxmap=tokenizer.labels_idxmap\n",
        "):\n",
        "  out_words, out_features, out_labels, sent_lengths = [], [], [], []\n",
        "  pad_and_feats = (\n",
        "      vocab_idxmap[PAD], tuple([0] * len(indexed_dataset[0][0][0][1]))\n",
        "  )\n",
        "  vocab_size = len(vocab_idxmap)\n",
        "  for sent in indexed_dataset:\n",
        "    N = len(sent)\n",
        "    sent_lengths.append(N)\n",
        "    for i in range(N):\n",
        "      (word, feats), label = sent[i]\n",
        "      \n",
        "      context_words = []\n",
        "      context_feats = []\n",
        "      for idx in range(i - n_pre, i+1 + n_post):\n",
        "        if idx == i: continue\n",
        "        c_word, c_feats = pad_and_feats if idx < 0 or idx >= N else sent[idx][0]\n",
        "        context_words.append(c_word)\n",
        "        context_feats.extend(list(c_feats))\n",
        "      out_words.append([word] + context_words)\n",
        "      out_features.append(list(feats) + context_feats)\n",
        "      out_labels.append(label)\n",
        "\n",
        "  categories = np.array([range(vocab_size)] * (1 + n_pre + n_post))\n",
        "  enc = OneHotEncoder(categories=categories)\n",
        "  out_dataset = enc.fit_transform(out_words)\n",
        "  assert (enc.categories_ == categories).all().all(), \"This can't change...\"\n",
        "\n",
        "  out_dataset = hstack((out_dataset, csr_matrix(out_features)), format='csr')\n",
        "  out_labels = [labels_idxmap[l if l in labels_idxmap else PAD] for l in out_labels]\n",
        "\n",
        "  return out_dataset, out_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKEqAYt9wUgp"
      },
      "source": [
        "##Checking shape of train dataset and number of labels\n",
        "train_sparse_X, train_Y = to_onehot_ngram(train_tokenized_index, n_pre=2)\n",
        "dev_sparse_X, dev_Y = to_onehot_ngram(tokenizer.transform(dev), n_pre=2)\n",
        "print(\"Generated train dataset of type %s, shape %s, with %d labels\" % (\n",
        "  type(train_sparse_X), str(train_sparse_X.shape), len(train_Y)\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPQA6E0vxxog"
      },
      "source": [
        "%%time\n",
        "##Training  model\n",
        "short_model = LogisticRegression(\n",
        "    solver='liblinear',\n",
        "    random_state=1,\n",
        "    penalty='l2',\n",
        "    C=0.1,\n",
        "    max_iter=25,\n",
        ")\n",
        "short_model.classes_ = list(range(tokenizer.num_classes))\n",
        "\n",
        "short_model.fit(train_sparse_X, train_Y)\n",
        "\n",
        "acc = short_model.score(dev_sparse_X, dev_Y)\n",
        "print(\"Model accuracy (*not* Macro F1): %.2f%%\" % (100 * acc))\n",
        "\n",
        "dev_probs = short_model.predict_proba(dev_sparse_X)\n",
        "new_F1 = f1_score(dev_probs.argmax(axis=1), dev_Y, average='macro')\n",
        "print(\"The improved baseline Macro F1: %.2f\" % new_F1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n2dIUZrwbgW"
      },
      "source": [
        "##TODO: Problem 2.2 - report the test F1 score obtained using the more complex,\n",
        "##trigram baseline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeEsJQKhEbaH"
      },
      "source": [
        "##TODO: Problem 2.3 - hyperparameter tuning for trigram model. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC2m2GVHOdGq"
      },
      "source": [
        "## Using a sequential model\n",
        "For this section, we'll refine our approach to the above task by using a sequential, natural language processing model. This task _does_ rely on the same datasets used in the previous part, and some of the code used earlier as well, so if you've restarted your environment for some reason, make sure you re-run those cells. Additionally, unlike in the prior section, here you will need to fill in some missing code blocks (preceeded by large, obvious comments in the code) in order for things to run and work properly. So, the code below *__will not__* run as it stands, but should run seamlessly once you've fixed it according to the provided instructions.\n",
        "\n",
        "Note that this section extensively uses PyTorch, one of the most popular current deep learning libraries. We will _not_ explain introductory PyTorch concepts here; instead, we expect you to be roughly familiar with the basics of deep learning software, and to supplement your background with external resources as needed to be comfortable with basic PyTorch. Some good options for external resources are the PyTorch Tutorials, available [here](https://pytorch.org/tutorials/). We will, however, explain what we're doing, and offer a brief outline of the classes we use, design decisions we make, and common patterns or antipatterns in PyTorch design.\n",
        "\n",
        "Similarly, we also do not explain in depth the theory behind the model we ultimately use here, a Gated Recurrent Unit (GRU) recurrent neural network (RNN) model. If you'd like to learn more about GRUs, they are explained in more depth in [this](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) blogpost, as well as (subordinately to LSTMs, but still) in [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) excellent post by Cristopher Olah.\n",
        "\n",
        "Now, onto some pytorch boilerplate.\n",
        "### PyTorch Datasets\n",
        "PyTorch's `torch.utils.data.Dataset` class and corresponding `torch.utils.data.dataloader` class are the primary ways to encapsulate your data for use in a PyTorch neural network model. The `Dataset` class is a base class, with two mandatory abstract methods: `__len__` and `__getitem__`. As its name indicates, `__len__` must return the _length_ of the dataset stored in your subclass---i.e., the number of samples (_not_ the number of batches or anything like this, the raw number of samples in total). Similarly well-named, `__getitem__` takes an integral index and must return pytorch tensors corresponding to the sample at that index in the data. Note that there _are_ constraints on the shapes of the tensors returned by `__getitem__`; namely, they must match for all indices passed to `__getitem__` (unless you define a specialized collate function, which we do not here). This is because the PyTorch dataloader utilities will call `__getitem__` for each element of a batch and collate the results into batched tensors. In general, this fact also implies it is wise to push as much of the processing in your `Dataset` subclass outside of the `__getitem__` path as possible so it isn't run each sample of your batch, and is instead run once (e.g., at construction) then stored for repeated use.\n",
        "\n",
        "In this assignment, we've nearly completely defined the dataset for you; however, you *__will__* have to modify things, as we task you with completing some broken code blocks. Accordingly, this code *__will not__* run as it is now--you'll need to fix it first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gosrjxTJMQlO"
      },
      "source": [
        "##Defining the Datset\n",
        "\n",
        "class NERDataset(data.Dataset):\n",
        "  def __init__(\n",
        "      self, dataset_list, tokenizer, max_len=None, already_indexed=False,\n",
        "      get_char_sequence=True, keep_chars_for_rare_words=True,\n",
        "      use_cuda=torch.cuda.is_available()\n",
        "  ):\n",
        "    \"\"\"\n",
        "    dataset_list: [ sentence 1: [((word_1, (auxiliary_features...)), label_1), ...], ...]\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.dataset_list = dataset_list\n",
        "    self.num_aux_features = len(tokenizer.auxiliary_features)\n",
        "    self.N = len(dataset_list)\n",
        "    self.__set_index(max_len)\n",
        "\n",
        "    self.vocab_idxmap = tokenizer.vocab_idxmap\n",
        "    assert UNK in self.vocab_idxmap and PAD in self.vocab_idxmap\n",
        "    self.labels_idxmap = tokenizer.labels_idxmap\n",
        "    assert PAD in self.labels_idxmap\n",
        "    self.use_cuda = use_cuda\n",
        "\n",
        "    assert not already_indexed and get_char_sequence, \"Can't do both.\"\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.get_char_sequence = get_char_sequence\n",
        "    self.keep_chars_for_rare_words = True\n",
        "    if self.get_char_sequence:\n",
        "      self.max_word_len = max(\n",
        "          len(w) for s in self.dataset_list for (w, _), _ in s\n",
        "      )\n",
        "        \n",
        "    self.already_indexed = already_indexed\n",
        "\n",
        "  def __set_index(self, global_max_len):\n",
        "    self.max_len = max(len(sent) for sent in self.dataset_list)\n",
        "    if global_max_len is None or self.max_len < global_max_len:\n",
        "      self.index = [(i, 0) for i in range(self.N)]\n",
        "      return\n",
        "\n",
        "    self.max_len = global_max_len\n",
        "    self.index = []\n",
        "    for i, sent in enumerate(self.dataset_list):\n",
        "      if len(sent) < self.max_len:\n",
        "        self.index.append((i, 0))\n",
        "        continue\n",
        "      \n",
        "      for idx in range(0, len(sent), self.max_len):\n",
        "        self.index.append((i, idx))      \n",
        "\n",
        "  def __len__(self): return len(self.index)\n",
        "\n",
        "  def __to_tensor(self, arr):\n",
        "    tens = torch.Tensor(arr).long()\n",
        "    if self.use_cuda: tens = tens.cuda()\n",
        "    return tens\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sent_idx, start_idx = self.index[idx]\n",
        "    sent = self.dataset_list[sent_idx][start_idx:start_idx+self.max_len]\n",
        "\n",
        "    X, C, C_mask, A, Y = [], [], [], [], []\n",
        "    # A simple modification here enables this to run on raw data.\n",
        "    for (w_orig, aux), l in sent:\n",
        "      w = w_orig\n",
        "\n",
        "      ## TODO: Task #1: ########################################################\n",
        "      ## For this task, you need to fill in the Dataset construction logic    ##\n",
        "      ## for this part of `__getitem__`. In each clause of the below if       ##\n",
        "      ## statement, some lines will be left incomplete. Fill them in,         ##\n",
        "      ## remembering what the point of this method is: Given an input         ##\n",
        "      ## sequence of words, return:                                           ##\n",
        "      ##   1) A tensor `mask` of shape [self.max_len,] containing a mask with ##\n",
        "      ##      1s indicating whether or not there was a real word present at   ##\n",
        "      ##      that position. This tensor is created later in the function,and ##\n",
        "      ##      you needn't worry about it here.                                ##\n",
        "      ##   2) A tensor `X` of shape [self.max_len,] containing an integral    ##\n",
        "      ##      index (based on the map from word -> index in                   ##\n",
        "      ##      `self.vocab_idxmap`) for each word in the sentence. You will    ##\n",
        "      ##      help create `X` here.                                           ##\n",
        "      ##   3) (If `self.get_char_sequence`): A tensor `C` of shape            ##\n",
        "      ##      [self.max_len, self.max_word_len] containing a sequence of      ##\n",
        "      ##      sequence of integral indexs corresponding to each letter in     ##\n",
        "      ##      each word (via the mapping in `self.tokenizer.chars_idxmap`).   ##\n",
        "      ##   4) (If `self.get_char_sequence`): A tensor `C_mask` of shape       ##\n",
        "      ##      [self.max_len, self.max_word_len] containing a sequence of      ##\n",
        "      ##      sequence of masks indicating which characters were present in   ##\n",
        "      ##      which words.                                                    ##\n",
        "      ##   5) A tensor `A` of shape [self.max_len, self.num_aux_features]     ##\n",
        "      ##      containing the auxiliary features per token. You won't need to  ##\n",
        "      ##      do anything with `A` here.                                      ##\n",
        "      ##   6) A tensor `Y` of shape [self.max_len,] containing the labels per ##\n",
        "      ##      token. You won't need to adjust `Y` here.                       ##\n",
        "      ## In this section, you'll need to help us construct some of these      ##\n",
        "      ## pieces.                                                              ##\n",
        "      ##########################################################################\n",
        "\n",
        "      if self.already_indexed: \n",
        "        # In this case, we don't need to change anything--the indexing has\n",
        "        # already been done.\n",
        "        X.append(w)\n",
        "      else: \n",
        "        # TODO: Fill this in. Here, we need to convert `w` to an index, using\n",
        "        # the map `self.vocab_idxmap`. Note that `w` may not be in this map, in\n",
        "        # which case we want to convert it to an UNK token before indexifying\n",
        "        # it. Once you've converted it, append it to `X`. Feel free to use\n",
        "        # helper variables.\n",
        "\n",
        "        X.append(...)\n",
        "      if self.get_char_sequence:\n",
        "        if self.keep_chars_for_rare_words: w_chars = w_orig\n",
        "        else: w_chars = w\n",
        "\n",
        "        if w_chars in (UNK, PAD, PUNCT): w_chars = ''\n",
        "        delta = self.max_word_len - len(w_chars)\n",
        "\n",
        "        # TODO: Fill this in. Here, we have `w_chars` and delta -- now we just\n",
        "        # need to convert the characters in w_chars to indices and pad the list\n",
        "        # to self.max_word_len.\n",
        "\n",
        "        cseq = ...\n",
        "        cseq += ...\n",
        "        C.append(cseq)\n",
        "\n",
        "        ## End Task #1 #########################################################\n",
        "\n",
        "        C_mask.append([1] * len(w_chars) + [0] * delta)\n",
        "\n",
        "      A.append(list(aux))\n",
        "\n",
        "      l = l if l in self.labels_idxmap else PAD #We ignore labels we don't know.\n",
        "      Y.append(self.labels_idxmap[l])\n",
        "    \n",
        "    delta = self.max_len - len(X)\n",
        "    if delta > 0:\n",
        "      mask = [1] * len(X) + [0] * delta\n",
        "      X.extend([self.vocab_idxmap[PAD]] * delta)\n",
        "      if self.get_char_sequence:\n",
        "        # Here, by construction, we rely on max_word_len being always larger\n",
        "        C.extend([[self.tokenizer.chars_idxmap[PAD]]*self.max_word_len] * delta)\n",
        "        C_mask.extend([[0]*self.max_word_len] * delta)\n",
        "      A.extend([[0] * self.num_aux_features] * delta)\n",
        "      Y.extend([self.labels_idxmap[PAD]] * delta)\n",
        "    else:\n",
        "      mask = [1] * self.max_len\n",
        "      X = X[:self.max_len]\n",
        "      A = A[:self.max_len]\n",
        "      if self.get_char_sequence:\n",
        "        C = C[:self.max_len]\n",
        "        C_mask = C_mask[:self.max_len]\n",
        "      Y = Y[:self.max_len]\n",
        "\n",
        "    if self.get_char_sequence:\n",
        "      return (\n",
        "          self.__to_tensor(mask), self.__to_tensor(X), self.__to_tensor(C),\n",
        "          self.__to_tensor(C_mask), self.__to_tensor(A).float(),\n",
        "          self.__to_tensor(Y)\n",
        "      )\n",
        "    else:\n",
        "      return (\n",
        "          self.__to_tensor(mask), self.__to_tensor(X),\n",
        "          self.__to_tensor(A).float(), self.__to_tensor(Y)\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT4xQh-oNoC5"
      },
      "source": [
        "To see what the dataset produces, we can instantiate one here and call it, using our tokenizer class from 2.1 here as well. Calling `train_dataset[0]` at the end (equivalent to `train_dataset.__getitem__(0)`) returns the tensors corresponding to the first example in our train dataset. Note these tensors are all padded out to the passed `max_len` (50), which is necessary here so all entries collate properly. The entries in the padding positions will be ignored in both model running and loss calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVCWfhkwxkvY"
      },
      "source": [
        "## \n",
        "nn_tokenizer = Tokenizer(rare_words_thresh=10)\n",
        "nn_tokenizer.fit(train)\n",
        "train_tokenized = nn_tokenizer.transform(train, as_index=False)\n",
        "\n",
        "train_dataset = NERDataset(\n",
        "    train_tokenized, nn_tokenizer, max_len=50, already_indexed=False\n",
        ")\n",
        "\n",
        "for i in train_dataset[0]:\n",
        "  print(i.shape)\n",
        "  display(i[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "albci3Cjjbjq"
      },
      "source": [
        "### Model Class & Training Loop\n",
        "\n",
        "Now onto the model class and training loop logic for this problem. Note that here, you *__will__* have to modify things, as we task you with completing some broken code blocks. Accordingly, this code *__will not__* run as it is now--you'll need to fix it first. We'll give a brief overview of each section here, and have comments within the code. \n",
        "\n",
        "#### Model Class\n",
        "Pytorch model classes, also called _modules_, are all subclassed from `torch.nn.Module`, which contains much of the logic necessary for, among other things, automatic differentiation. There are different schools of thought on the best way to design modules, from an engineering perspective. One approach is to try to make many small modules, each of which is relatively isolated and encompasses only a small part of the model, then assemble them heirarchically into a larger whole. This approach is more modular and flexible, which can be very advantageous in research. Another is to have most of your model in a few larger modules, which will often go so far as to compute final losses or normalized probabilities straight from raw input. This approach argues that, with research use cases often requiring nuanced, individualized support, trying to re-use smaller modules risks making code more brittle and dependent on more special-cased support, and that it is better to stick with only the basic primitives provided by torch directly.\n",
        "\n",
        "Here, given our use case is narrow and limited, we've defined just one `nn.Module` subclass, called `NER_GRU`. This class, on `forward`, ingests `mask`, a tensor describing which words are actually present in this sentence, the tokenized word sequence `X`, a tensor `aux` containing the auxiliary features output by the tokenizer (e.g., capitalization status, etc.), and the final output labels (numerically encoded) in `labels`. It first embeds the words via a trainable, randomly initialized word embedding layer, adds to these embeddings the auxiliary features, projected into the embedding layer via a linear transformation, runs this sequence through a GRU parametrized by the hyperparameters passed, passes each sequential output of the GRU through a linear output layer, then scores the whole system with a `CrossEntropyLoss` against the provided labels. It also normalizes the per-token scores into probabilities via a `nn.Softmax` layer and passes these probabilities out as well as the loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYhqlkT82uoS"
      },
      "source": [
        "class SeqModelTypes(Enum):\n",
        "  NONE = auto()\n",
        "  GRU = auto()\n",
        "  LSTM = auto()\n",
        "\n",
        "class NER_Model(nn.Module):\n",
        "  SeqModelConstructors = {\n",
        "      SeqModelTypes.GRU: nn.GRU,\n",
        "      SeqModelTypes.LSTM: nn.LSTM,\n",
        "  }\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      tokenizer = nn_tokenizer,\n",
        "      embed_size = 50,\n",
        "      hidden_size = 150,\n",
        "      num_layers = 1,\n",
        "      dropout = 0.1,\n",
        "      bidirectional = False,\n",
        "      rnn_type=SeqModelTypes.LSTM,\n",
        "      char_rnn_type=SeqModelTypes.NONE, # if NONE, params below ignored.\n",
        "      char_embed_size=25,\n",
        "      char_rnn_hidden_size=50,\n",
        "      char_rnn_num_layers=1,\n",
        "      char_rnn_bidirectional=True,\n",
        "  ):\n",
        "    assert rnn_type != SeqModelTypes.NONE\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # We use the tokenizer as a convenient storage location for the vocab and\n",
        "    # label indexmapstest = read_dataset_file('2014_test.tsv'), as well as the number of auxiliary features.\n",
        "    self.vocab_idxmap = tokenizer.vocab_idxmap\n",
        "    self.labels_idxmap = tokenizer.labels_idxmap\n",
        "    self.num_aux_features = len(tokenizer.auxiliary_features)\n",
        "    self.char_idxmap = tokenizer.chars_idxmap\n",
        "\n",
        "    # Embedding hyperparameters\n",
        "    self.vocab_size = len(self.vocab_idxmap)\n",
        "    self.char_vocab_size = len(self.char_idxmap)\n",
        "    self.embed_size = embed_size\n",
        "    self.char_embed_size = char_embed_size\n",
        "\n",
        "    # GRU Hyperparameters\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = dropout\n",
        "    self.bidirectional = bidirectional\n",
        "    self.rnn_type = rnn_type\n",
        "    self.char_rnn_type = char_rnn_type\n",
        "    self.embed_char_seq = (self.char_rnn_type != SeqModelTypes.NONE)\n",
        "    self.char_rnn_hidden_size = char_rnn_hidden_size\n",
        "    self.char_rnn_num_layers = char_rnn_num_layers\n",
        "    self.char_rnn_bidirectional = char_rnn_bidirectional\n",
        "\n",
        "    # Output Hyperparameters\n",
        "    self.num_classes = len(self.labels_idxmap) - 1 # -1 for padding\n",
        "\n",
        "    # Model parts\n",
        "    self.embed_layer = nn.Embedding(\n",
        "        self.vocab_size, self.embed_size, padding_idx=self.vocab_idxmap[PAD],\n",
        "    )\n",
        "    if self.num_aux_features:\n",
        "      # We only need to produce this layer if we have any aux features...\n",
        "      self.aux_embed_layer = nn.Linear(self.num_aux_features, self.embed_size)\n",
        "    \n",
        "    sent_rnn_params = dict(\n",
        "      input_size    = self.embed_size,\n",
        "      hidden_size   = self.hidden_size,\n",
        "      num_layers    = self.num_layers,\n",
        "      batch_first   = True,\n",
        "      dropout       = self.dropout,\n",
        "      bidirectional = self.bidirectional,\n",
        "    )\n",
        "    self.rnn = NER_Model.SeqModelConstructors[rnn_type](**sent_rnn_params)\n",
        "\n",
        "    if self.embed_char_seq:\n",
        "      self.char_embed_layer = nn.Embedding(\n",
        "          self.char_vocab_size, self.char_embed_size,\n",
        "          padding_idx=self.char_idxmap[PAD]\n",
        "      )\n",
        "      char_rnn_params = dict(\n",
        "        input_size    = self.char_embed_size,\n",
        "        hidden_size   = self.char_rnn_hidden_size,\n",
        "        num_layers    = self.char_rnn_num_layers,\n",
        "        batch_first   = True,\n",
        "        dropout       = self.dropout,\n",
        "        bidirectional = self.char_rnn_bidirectional,\n",
        "      )\n",
        "\n",
        "      # Time distributed expects a simple module, which returns a tensor.\n",
        "      # Unfortunately, pytorch RNNs don't do this. So we use a virtual module\n",
        "      # to account for this.\n",
        "      class SimpleOutputRNN(nn.Module):\n",
        "        def __init__(self, rnn_cnstr, rnn_kwargs):\n",
        "          super().__init__()\n",
        "\n",
        "          self.rnn = rnn_cnstr(**rnn_kwargs)\n",
        "          if 'num_layers' in rnn_kwargs:\n",
        "            self.num_layers = rnn_kwargs['num_layers']\n",
        "          else: self.num_layers = 1\n",
        "          if 'bidirectional' in rnn_kwargs and rnn_kwargs['bidirectional']:\n",
        "            self.num_directions = 2\n",
        "          else: self.num_directions = 1\n",
        "\n",
        "          self.hidden_size = rnn_kwargs['hidden_size']\n",
        "\n",
        "        def forward(self, seq):\n",
        "          \"\"\"\n",
        "            `seq` is of shape (augmented batch_size, rnn_seq_len, hidden_dim)\n",
        "          \"\"\"\n",
        "          aug_batch_size, seq_len, _ = seq.shape\n",
        "          rnn_out = self.rnn(seq)[0]\n",
        "          # if type(rnn_out) is tuple: rnn_out = rnn_out[0]\n",
        "          return rnn_out.view(aug_batch_size, seq_len, -1)\n",
        " \n",
        "      self.char_rnn = TimeDistributed(SimpleOutputRNN(\n",
        "          NER_Model.SeqModelConstructors[self.char_rnn_type], char_rnn_params\n",
        "      ))\n",
        "\n",
        "      self.char_num_directions = (2 if self.char_rnn_bidirectional else 1)\n",
        "      char_rnn_out_dim = self.char_num_directions * self.char_rnn_hidden_size\n",
        "      self.char_seq_embed_layer = nn.Linear(char_rnn_out_dim, self.embed_size)\n",
        "\n",
        "    num_directions = (2 if self.bidirectional else 1)\n",
        "    rnn_out_dim = num_directions * self.hidden_size\n",
        "\n",
        "    self.output_layer = nn.Linear(rnn_out_dim, self.num_classes)\n",
        "\n",
        "    # Loss & Output normalization\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "    self.loss_fn = nn.CrossEntropyLoss(ignore_index = self.labels_idxmap[PAD])\n",
        "\n",
        "  def forward(self, *inputs):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      `mask` is of shape (batch_size, seq_len), contains 1 if word is present, 0\n",
        "        otherwise.\n",
        "      `X` is of shape (batch_size, seq_len), contains ids of words to be\n",
        "        embedded.\n",
        "      `C` is of shape (batch_size, seq_len, word_len), contains ids of chars\n",
        "        within words to be embedded. Only present if self.embed_char_seq.\n",
        "      `C_mask` is of shape (batch_size, seq_len, word_len), contains 1 if char\n",
        "        is present, 0 otherwise. Only present if self.embed_char_seq.\n",
        "      `aux` is of shape (batch_size, seq_len, self.num_aux_features), contains any\n",
        "        auxiliary features computed by the tokenizer (e.g., is_capitalized)\n",
        "      `labels` is of shape (batch_size, seq_len), contains ids of labels per\n",
        "        token.\n",
        "    Outputs:\n",
        "      `probs` is of shape (batch_size, seq_len, self.num_classes). Contains\n",
        "        normalized probabilities per token, per class. All probabilities for\n",
        "        padding tokens have been zeroed out.\n",
        "      `loss` is a scalar, containing the mean cross entropy loss across all\n",
        "        tokens and all sentences. Note that this loss doesn't do anything clever\n",
        "        w.r.t. normalizing for sentence length --- i.e., it is not a \n",
        "        averaged per-sentence/per-example loss, but rather a per-token loss. For\n",
        "        our purposes this is fine.\n",
        "    \"\"\"\n",
        "\n",
        "    if self.embed_char_seq:\n",
        "      mask, X, C, C_mask, aux, labels = inputs\n",
        "    else:\n",
        "      mask, X, _, _, aux, labels = inputs\n",
        "\n",
        "    # For this problem, we ask you to fill in part of the forward method of this\n",
        "    # neural network. Recall our stated plan.\n",
        "    #  1. First, we're going to embed the input `X` (recall we've initialized\n",
        "    #     an embedding layer already in `self.embed_layer`.\n",
        "    #  2. Next, if there are any auxiliary features, we'll project those into\n",
        "    #     into the embedding space via our projection layer\n",
        "    #     `self.aux_embed_layer`, and add them to the embedded words.\n",
        "    #  3. Embed the character sequence. This will be a miniature version of our\n",
        "    #     whole pipeline. First, we embed them according to the embedding layer\n",
        "    #     `self.char_embed_layer`, then we pass them through our (modified)\n",
        "    #     `self.char_rnn`, followed by the char rnn output layer \n",
        "    #     `self.char_seq_embed_layer`. Finally, we add them to our embedded\n",
        "    #     words \n",
        "    #  4. We pass this featurized embedding tensor through `self.rnn`, keeping\n",
        "    #     the output at each token via the first element of the pytorch RNN\n",
        "    #     output tuple.\n",
        "    #  5. We send these per-token output through `self.output_layer` to get our \n",
        "    #     scores.\n",
        "    #  6. Finally, we compute our loss and probabilities, and return them.\n",
        "\n",
        "    # First we embed, always the words and the auxiliary features as well if\n",
        "    # present.\n",
        "    token_features = []\n",
        "    token_features.append(self.embed_layer(X))\n",
        "\n",
        "    if self.num_aux_features: \n",
        "      token_features.append(self.aux_embed_layer(aux.float()))\n",
        "\n",
        "    if self.embed_char_seq:\n",
        "      # Now we embed the character sequence\n",
        "      embedded_chars = self.char_embed_layer(C)\n",
        "      char_rnn_out = self.char_rnn(embedded_chars)\n",
        "      # `char_rnn_out` is of shape (batch, seq, word_len, hidden_dim)\n",
        "      # We take an average pooling over the word-level, subject to C_mask.\n",
        "\n",
        "      # `C_mask` is of shape (batch, seq, word_len)\n",
        "      char_rnn_mask = C_mask.unsqueeze(3).expand_as(char_rnn_out)\n",
        "      char_rnn_out = (char_rnn_out * char_rnn_mask).sum(dim=2)\n",
        "      word_lens = C_mask.sum(dim=2).unsqueeze(2).expand_as(char_rnn_out)\n",
        "      \n",
        "      # Here, we're really just computing char_rnn_out / word_lens. Except,\n",
        "      # for padding words, word_len can be 0, so we need to catch that case,\n",
        "      # and zero it out, then divide by something that looks like word_lens\n",
        "      # otherwise or is 1 to avoid illusory nans. We use torch.where to catch\n",
        "      # these conditionals.\n",
        "      word_lens_or_one = torch.where(\n",
        "          word_lens == 0, torch.ones_like(word_lens), word_lens\n",
        "      )\n",
        "      char_rnn_out = torch.where(\n",
        "          word_lens == 0, torch.zeros_like(char_rnn_out),\n",
        "          char_rnn_out / word_lens_or_one\n",
        "      )\n",
        "\n",
        "      char_rnn_out = self.char_seq_embed_layer(char_rnn_out)\n",
        "\n",
        "      token_features.append(char_rnn_out)\n",
        "\n",
        "    #### TODO: Task #2: ########################################################\n",
        "    ##   Combine all the per-token features together, then pass them through  ##\n",
        "    ##   `self.rnn` and `self.output_layer` to get our per-token, per-class   ##\n",
        "    ##   scores.                                                              ##\n",
        "    ##   *Hint:* Remember that PyTorch's RNN methods return tuples, and you   ##\n",
        "    ##   only need part of the tuple here.                                    ##\n",
        "    ############################################################################\n",
        "    embedded_token_seq = # TODO: Fill this part in!\n",
        "    rnn_out = # TODO: Fill this part in!\n",
        "    scores = # TODO: Fill this part in!\n",
        "    #### End Task #2 ###########################################################\n",
        "\n",
        "\n",
        "    # scores is of shape (batch_size, seq_len, num_classes), but the loss\n",
        "    # expects data of the shape (batch_size, num_classes, d_1, d_2, ...) and\n",
        "    # labels of the shape (batch_size, d_1, d_2, ...), so we need to reshape:\n",
        "    scores_reshaped = scores.transpose(1, 2)\n",
        "    labels_reshaped = labels # This is actually fine as is\n",
        "    loss = self.loss_fn(scores_reshaped, labels_reshaped)\n",
        "\n",
        "    # We comptue probabilities here for convenience later.\n",
        "    probs = self.softmax(scores) * mask.unsqueeze(2).expand_as(scores)\n",
        "    \n",
        "    return probs, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E638a6_qo4gJ"
      },
      "source": [
        "#### Train & Evaluation Loops\n",
        "We also need to write our training and evaluation loops here. Note that there are libraries to automate some of this logic, most notably PyTorch's own [Ignite](https://pytorch.org/ignite/), which we do not use here for simplicity but are very useful for larger projects.\n",
        "\n",
        "Our train and evaluation loops are simple. In the `eval_model` function, we simply create a deterministic (i.e., `shuffle=False`) `DataLoader` over the dev set, then iterate through this dataloader and collate the final probabilities. These are then re-structured to match the original data formatting and passed into the `ner_eval` function from part 1. We default to large `batch_size` in this function because larger batches can afford greater efficiencies in terms of GPU usage, and the validation batch size will not affect performance as no gradients are computed during evaluation.\n",
        "\n",
        "In the `run_model` function, we use a very similar setup the `eval_model` function just described, save that we run over the train dataset many times, once per epoch, optimizing the parameters via the [`Adam` optimizer](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) each batch. Each epoch, we also evaluate the model on the validation data, so that we can track final performance. Note the hyperparameters passed to this function:\n",
        "  * `num_epochs`\n",
        "  * `batch_size`\n",
        "  * `learning_rate_init`\n",
        "\n",
        "All of these can have significant impacts on learning speed and final performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr8M0yLu-Vnk"
      },
      "source": [
        "def eval_model(val_dataset, model, batch_size = 1024, debug=False):\n",
        "  val_dataloader = data.DataLoader(\n",
        "      val_dataset, batch_size=batch_size, shuffle=False\n",
        "  )\n",
        "  num_val_batches = len(val_dataloader)\n",
        "\n",
        "  model.eval()\n",
        "  val_batches = tqdm(\n",
        "      val_dataloader, total=num_val_batches, leave=False, desc=\"Val Batch\"\n",
        "  )\n",
        "  val_probs, val_probs_flat = [], []\n",
        "  val_labels, val_labels_flat = [], []\n",
        "  val_loss = 0\n",
        "  for batch in val_batches:\n",
        "    with torch.no_grad():\n",
        "      probs, loss = model(*batch)\n",
        "      val_loss += loss.detach().cpu().numpy()\n",
        "      probs = list(probs.detach().cpu().numpy())\n",
        "      mask = list(batch[0].detach().cpu().numpy())\n",
        "      labels = list(batch[-1].detach().cpu().numpy())\n",
        "    probs_ragged, probs_flat = [], []\n",
        "    labels_ragged, labels_flat = [], []\n",
        "    for probs_sent, mask_sent, labels_sent in zip(probs, mask, labels):\n",
        "      probs_ragged.append(probs_sent[:sum(mask_sent)])\n",
        "      labels_ragged.append(labels_sent[:sum(mask_sent)])\n",
        "      probs_flat.extend(probs_sent[:sum(mask_sent)])\n",
        "      labels_flat.extend(labels_sent[:sum(mask_sent)])\n",
        "    val_probs.extend(probs_ragged)\n",
        "    val_labels.extend(labels_ragged)\n",
        "    val_probs_flat.extend(probs_flat)\n",
        "    val_labels_flat.extend(labels_flat)\n",
        "\n",
        "  if debug: \n",
        "      return val_probs, val_dataset.dataset_list, val_dataset.labels_idxmap\n",
        "\n",
        "  val_loss /= num_val_batches\n",
        "  val_f1 = f1_score(val_labels_flat, np.argmax(val_probs_flat, axis=1), average='macro')\n",
        "\n",
        "  return val_loss, val_f1\n",
        "\n",
        "def run_model(\n",
        "    train_dataset, val_dataset, model,\n",
        "    num_epochs = 20,\n",
        "    batch_size = 64,\n",
        "    learning_rate_init = 3e-4,\n",
        "    val_batch_size=512,\n",
        "    step_size = 10,\n",
        "    gamma = 0.1,\n",
        "):\n",
        "  train_dataloader = data.DataLoader(\n",
        "      train_dataset, batch_size=batch_size, shuffle=True\n",
        "  )\n",
        "  num_train_batches = len(train_dataloader)\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=learning_rate_init)\n",
        "  sch = optim.lr_scheduler.StepLR(\n",
        "      optimizer, step_size, gamma=gamma, last_epoch=-1\n",
        "  )\n",
        "\n",
        "  epoch_progress_temp = \"Epoch: Train Loss %.2e, Dev F1 %.2f\"\n",
        "  batch_progress_temp = \"Train Batch: %.2e\"\n",
        "\n",
        "  epochs = tqdm(\n",
        "      range(num_epochs), desc=epoch_progress_temp % (np.NaN, np.NaN)\n",
        "  )\n",
        "  val_f1s = []\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  for epoch in epochs:\n",
        "    model.train()\n",
        "    \n",
        "    train_batches = tqdm(\n",
        "        train_dataloader, total=num_train_batches, leave=False,\n",
        "        desc=batch_progress_temp % (np.NaN),\n",
        "    )\n",
        "    losses_epoch = []\n",
        "    for batch in train_batches:\n",
        "      ## TODO: Task #3: ########################################################\n",
        "      ## Here, you need to compute the actual training loop for this train    ##\n",
        "      ## function. Note that this section of code is relatively model         ##\n",
        "      ## agnostic. All you need to do is zero the gradients stored in our     ##\n",
        "      ## optimizer, compute the loss of the model on the batch (recall our    ##\n",
        "      ## model outputs both the probabilities and the loss on each call to    ##\n",
        "      ## forward, run the `backward` step of PyTorch's automatic              ##\n",
        "      ## differentiation system on the loss tensor, then `step` with the      ##\n",
        "      ## optimizer to adjust the model parameters.                            ##\n",
        "      ## Go through some basic PyTorch tutorials if this is unfamiliar to     ##\n",
        "      ## you. A paritcularly good place to look could be here:                ##\n",
        "      ## https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html,  ##\n",
        "      ## in particular, Section 4.\n",
        "      ##########################################################################\n",
        "\n",
        "      # Complete the optimizer here. Store the loss in a tensor called `loss`\n",
        "      # or the rest of the function will not work.\n",
        "\n",
        "      ## End Task #3 ###########################################################\n",
        "\n",
        "      loss_value = loss.detach().cpu().numpy()\n",
        "      train_batches.set_description(batch_progress_temp % loss_value)\n",
        "      losses_epoch.append(loss_value)\n",
        "\n",
        "    val_loss, val_f1 = eval_model(val_dataset, model, val_batch_size)\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_f1s.append(val_f1)\n",
        "    train_losses.append(losses_epoch)\n",
        "\n",
        "    epochs.set_description(\n",
        "        epoch_progress_temp % (np.mean(losses_epoch[-20:]), val_f1)\n",
        "    )\n",
        "    sch.step()\n",
        "\n",
        "  return model, train_losses, val_losses, val_f1s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC9s13UdqmBT"
      },
      "source": [
        "#### Training our model\n",
        "\n",
        "Finally, with all that done, it's time to actually train our model. Note two things:\n",
        "\n",
        "For training purposes, we set the maximum length of any sentence over which we can train to 50 tokens. But, at validation time, we leave it unset, allowing the system to span sentences much longer. Is this sensible? What impact might this have on our training?\n",
        "This cell will take a fairly long time (e.g., ~60 minutes) to train/run. It will print output after the datasets are constructed, then show a progress bar (if this does not appear to be working properly, ensure you are using the most recent version of chrome, and recall that you should've restarted the runtime once after running the very first import cell of this notebook) during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsTvyjSHSDS7"
      },
      "source": [
        "%%time\n",
        "nn_tokenizer = Tokenizer(\n",
        "  rare_words_thresh=RARE_WORDS_THRESHOLD,\n",
        "  auxiliary_features=[],\n",
        "  lowercase=True,\n",
        "  remove_punct=False,\n",
        "  replace_only_punct=False,\n",
        "  replace_numbers=NUM,\n",
        ")\n",
        "nn_tokenizer.fit(train)\n",
        "train_tokenized = nn_tokenizer.transform(train, as_index=False)\n",
        "dev_tokenized = nn_tokenizer.transform(dev, as_index=False)\n",
        "\n",
        "train_dataset = NERDataset(\n",
        "    train_tokenized, nn_tokenizer, max_len=50, already_indexed=False\n",
        ")\n",
        "val_dataset = NERDataset(\n",
        "    dev_tokenized, nn_tokenizer, max_len=None, already_indexed=False\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Produced train and validation datasets. \"\n",
        "    \"Validation max_len is %d\" % val_dataset.max_len\n",
        ")\n",
        "\n",
        "model = NER_Model(\n",
        "  tokenizer=nn_tokenizer,\n",
        "  embed_size=300,\n",
        "  dropout=0.5,\n",
        "  hidden_size=450,\n",
        "  num_layers=1,\n",
        "  bidirectional=True,\n",
        "  rnn_type = SeqModelTypes.LSTM,\n",
        "  char_rnn_type = SeqModelTypes.LSTM,\n",
        ")\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "model, train_losses, val_losses, val_f1s = run_model(\n",
        "    train_dataset, val_dataset, model, num_epochs=20, learning_rate_init=3e-4,\n",
        "    gamma = 0.5, val_batch_size=256\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9bDvEczhWV7"
      },
      "source": [
        "##TODO: Problem 3.1 - Evaluate the final performance on the test-set."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak5a0YjPhu3F"
      },
      "source": [
        "##TODO: Problem 3.2 - testing model sensitivity on parameters "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}